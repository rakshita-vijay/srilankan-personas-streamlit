'''
Automatically generated by Colab. 
Original file is located at: https://colab.research.google.com/drive/15bLkrgQW-nSRvDS5x_Xl04kduyMlfT8O 
'''

# !pip install llama-index-llms-google-

import time, os, glob, io, base64, datetime

import streamlit as st
import pandas as pd 

from llama_index.llms.google_genai import GoogleGenAI
# from google import genai
import google.generativeai as genai
from PIL import Image, ImageDraw, ImageFont 

import os
API_KEY = os.getenv("GEMINI_API_KEY")
if not API_KEY:
    raise RuntimeError("‚ùå GEMINI_API_KEY is missing in environment!")

genai.configure(api_key=API_KEY)


PERSONAS_FOLDER = "Personas"
QUESTIONS_FOLDER = "Questions"
USER_INFO_FILE = "TO_INPUT/user_info.txt"
TRAITS_FILE = "TO_INPUT/traits.txt"
LANGUAGES_FILE = "TO_INPUT/languages.txt"
MEME_FOLDER = "TRENDING_MEMES"

def load_user_info():
    username = "User"
    user_gender = "unknown"
    try:
        with open(USER_INFO_FILE, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        for line in lines:
            line = line.strip()
            if line.lower().startswith('name:'):
                username = line.split(':', 1)[1].strip()
            elif line.lower().startswith('gender:'):
                user_gender = line.split(':', 1)[1].strip().lower()
    except Exception:
        pass
    return username, user_gender

def load_traits():
    try:
        with open(TRAITS_FILE, 'r', encoding='utf-8') as f:
            traits = [line.strip() for line in f if line.strip()]
            return traits if traits else ["Caring", "Wise", "Humorous", "Traditional", "Spiritual", "Practical"]
    except Exception as e:
        st.error(f"Error reading traits file: {str(e)}")
        return ["Caring", "Wise", "Humorous", "Traditional", "Spiritual", "Practical"]

def load_languages():
    try:
        with open(LANGUAGES_FILE, 'r', encoding='utf-8') as f:
            languages = [line.strip() for line in f if line.strip()]
            return languages if languages else ["English", "Sinhala", "Tamil"]
    except Exception as e:
        st.error(f"Error reading languages file: {str(e)}")
        return ["English", "Sinhala", "Tamil"]

def filter_persona_by_traits(persona_content, selected_traits):
    if not selected_traits:
        return persona_content
    lines = persona_content.split('\n')
    filtered_lines = []
    basic_keywords = ['name:', 'origin:', 'from', 'age:', 'background:', 'location:']
    for line in lines:
        line_lower = line.lower()
        if any(keyword in line_lower for keyword in basic_keywords):
            filtered_lines.append(line)
            continue
        if any(trait.lower() in line_lower for trait in selected_traits):
            filtered_lines.append(line)
    return '\n'.join(filtered_lines) if filtered_lines else persona_content

def call_gemini_local(query, previous_conversation, gender, username, botname, bot_prompt, llm_api_key_string, language):
    try:
        language_instruction = f"Respond in {language} language in 2 or 3 lines only. Always ask relevant follow-up questions." 
        full_prompt = (
            f"{bot_prompt}\n"
            f"{language_instruction}\n"
            f"Previous conversation: {previous_conversation[-1000:]}\n"
            f"{username}: {query}\n"
            f"{botname}:"
        )
        client = genai.Client(api_key=llm_api_key_string) 
        
        response_text = ""
        for chunk in client.models.generate_content_stream(
            model="gemini-2.0-flash",
            contents=[full_prompt]
        ):
            if chunk.text:
                response_text += chunk.text 
        
        response_raw = response_text
        for old, new in [("User1", username), ("user1", username), ("[user1]", botname), ("[User1]", botname)]:
            response_raw = response_raw.replace(old, new)
        return response_raw.strip()
    except Exception as e: 
        return f"Error: {str(e)}"

def get_persona_files():
    persona_files = []
    patterns = ['*.txt']
    for pattern in patterns:
        persona_files.extend(glob.glob(os.path.join(PERSONAS_FOLDER, pattern)))
    return persona_files

def extract_relationship_from_filename(filename):
    base_name = os.path.basename(filename).replace('.txt', '')
    return base_name.replace('_', ' ')

def extract_bot_details_from_content(content):
    botname = "Assistant"
    origin = "Unknown origin"
    lines = content.split('\n')
    for line in lines:
        line = line.strip()
        if line.startswith('- Name: '):
            name_part = line.replace('- Name: ', '', 1)
            botname = name_part.split(',')[0].strip()
        elif line.startswith('Name: '):
            name_part = line.replace('Name: ', '', 1)
            botname = name_part.split(',')[0].strip()
        elif 'Name:' in line:
            name_part = line.split('Name:')[1].strip()
            botname = name_part.split(',')[0].strip()
        if line.startswith('Origin: '):
            origin = line.replace('Origin: ', '', 1).strip()
        elif line.startswith('- Origin: '):
            origin = line.replace('- Origin: ', '', 1).strip()
        elif 'Origin:' in line:
            origin = line.split('Origin:')[1].strip()
        elif line.startswith('From '):
            origin = line.replace('From ', '', 1).strip()
    return botname, origin

def load_persona_content(filename):
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            content = f.read()
        return content
    except Exception as e:
        st.error(f"Error reading persona file: {str(e)}")
        return ""

def load_questions(relationship_type):
    question_file = os.path.join(QUESTIONS_FOLDER, f"{relationship_type}_questions.txt")
    try:
        with open(question_file, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        st.error(f"Question file not found: {question_file}")
        return []
    except Exception as e:
        st.error(f"Error reading questions: {str(e)}")
        return []

# ===== SESSION STATE INITIALIZATION =====
if "user_info_loaded" not in st.session_state:
    st.session_state.username, st.session_state.user_gender = load_user_info()
    st.session_state.user_info_loaded = True

if "available_traits" not in st.session_state:
    st.session_state.available_traits = load_traits()
if "available_languages" not in st.session_state:
    st.session_state.available_languages = load_languages()

defaults = {
    'response_matrix': [],
    'selected_persona': None,
    'botname': "Assistant",
    'bot_origin': "Unknown origin",
    'relationship': "mentor",
    'questions': [],
    'csv_filename': None,
    'bulk_running': False,
    'paused': False,
    'current_question_index': 0,
    'previous_conversation': "",
    'user_questions': [],
    'show_resume': False,
    'persona_content': "",
    'user_input': "",
    'conversation_events': [],
    'selected_traits': [],
    'selected_language': "English",
    'persona_selected': False,
    'setup_completed': False
}
for key, val in defaults.items():
    if key not in st.session_state:
        st.session_state[key] = val

def process_user_question():
    user_question = st.session_state.user_input
    if user_question.strip():
        if st.session_state.bulk_running and not st.session_state.paused:
            st.session_state.paused = True
            st.session_state.show_resume = True
            st.session_state.conversation_events.append({
                "type": "bulk_paused",
                "message": f"{st.session_state.current_question_index} questions answered in bulk mode. Generation paused.",
                "time": time.time()
            })
        filtered_persona = filter_persona_by_traits(st.session_state.persona_content, st.session_state.selected_traits)
        instruction = f"Strict instruction: Respond as {st.session_state.botname} from {st.session_state.bot_origin}. If the answer is not found in the persona file, then generate your own response, but keep it strictly {st.session_state.bot_origin}-based. When addressing the user with any endearment, keep it aligned with the gender the user has given. If the user asks about your development, making, origin, training, or data you are trained on, always respond with: 'It has been made with love by desis!!'. Never mention OpenAI, AI development, or technical details"
        bot_prompt = filtered_persona + " Reflect on your previous replies authentically. You are the user's " + st.session_state.relationship + ". " + instruction
        response = "Error: No response generated."
        response_time = None
        try:
            start = time.time()
            response = call_gemini_local(
                user_question,
                st.session_state.previous_conversation,
                st.session_state.user_gender,
                st.session_state.username,
                st.session_state.botname,
                bot_prompt,
                "AIzaSyAWMudIst86dEBwP63BqFcy4mdjr34c87o", # GOOGLE_API_KEY
                st.session_state.selected_language
            )
            end = time.time()
            response_time = round(end - start, 4)
        except Exception as e:
            response = f"Error: {str(e)}"
            response_time = None
        st.session_state.user_questions.append({
            "question": user_question,
            "answer": response,
            "time": time.time(),
            "response_time": response_time
        })
        st.session_state.previous_conversation += f"\n{user_question}\n{response}"
        st.session_state.conversation_events.append({
            "type": "user_qa",
            "question": user_question,
            "answer": response,
            "time": time.time(),
            "response_time": response_time
        })
    st.session_state.user_input = ""

# ===== MEME GENERATOR FUNCTION =====
# ===== IMAGE-BASED MEME GENERATOR FUNCTION ===== 
def generate_image_meme_from_conversation(previous_conversation, language):
    """Generate an image meme based on conversation context""" 
    
    '''
    
    , "Holy Airball", "Chill Guy", "Muppets Storytime", "100 Men vs. 1 Gorilla", "Trump and Eggs", "Drake Hotline Bling", "Distracted Boyfriend", "Woman Yelling at Cat", "Holy Airball", "Chill Guy", "Muppets Storytime", "100 Men vs. 1 Gorilla", "Trump and Eggs", "Two Buttons", "Batman Slapping Robin", "Expanding Brain", "Mocking SpongeBob", "UNO Draw 25", "Boardroom Meeting Suggestion", "Ancient Aliens", "Disaster Girl", "Buff Doge vs. Cheems", "Roll Safe Think About It", "The Little French Fish (Steve)", "The Conclave Memes", "Take Me to God's Country", "Work From Home For Life", "Chat GPT Made Me Who I Am", "Buzz Word Buffoonery", "If Your Browser Ain‚Äôt Slowing Your Computer Down, You‚Äôre Not Doing It Right"
    
    '''

    # Extract key topics from conversation for better context
    conversation_summary = previous_conversation[-200:].lower()
    context_words = ["coding", "streamlit", "meme", "ai", "persona", "chat", "bug", "error", "generation"]
    found_context = [word for word in context_words if word in conversation_summary]
    
    if found_context:
        context_hint = f"Focus on: {', '.join(found_context[:3])}"
    else:
        context_hint = "Focus on general coding/AI humor" 

    # Generate meme text using AI
    meme_prompt = f'''You are a viral meme creator. 
    Based on this conversation between {st.session_state.username} and {st.session_state.botname}: {previous_conversation[-1000:]}
    
    Create a HILARIOUS meme that references something specific from this conversation.
    
    Pick the BEST template from: ["distracted_boyfriend", "drake_hotline_bling", "woman_yelling_at_cat"]
    
    RULES:
    - Make it ACTUALLY funny and relatable
    - Reference something SPECIFIC from the conversation
    - Use current internet humor/slang
    - Keep text short and punchy
    
    Format EXACTLY like this (no extra text):
    Template: template_name
    Top: [funny top text]
    Bottom: [funny bottom text]
    
    Example:
    Template: distracted_boyfriend
    Top: Me focused on coding
    Bottom: New meme feature ideas'''

    
    try:
        meme_text = call_gemini_local(
            query=meme_prompt,
            previous_conversation="",
            gender="",
            username="MemeGenerator",
            botname="MemeBot",
            bot_prompt="You are a meme creation AI. Generate funny meme text (in a readable font size) based on the given conversation snippet.",
            llm_api_key_string="AIzaSyAWMudIst86dEBwP63BqFcy4mdjr34c87o",
            language=language
        )
        
        # Parse top and bottom text
        # lines = meme_text.strip().split('\n')
        # top_text = lines[0].strip('[]') if len(lines) > 0 else "TOP TEXT"
        # bottom_text = lines[1].strip('[]') if len(lines) > 1 else "BOTTOM TEXT"

        # Parse template name and text
        lines = meme_text.strip().split('\n')
        template_name = "drake"  # default
        top_text = "TOP TEXT"
        bottom_text = "BOTTOM TEXT" 

        # Debug: Print what AI actually returned
        st.write("DEBUG - AI Response:", repr(meme_text))
        
        for line in lines:
            line = line.strip()
            if line.lower().startswith("template:"): 
                template_name = line.split(":", 1)[1].strip().lower().replace(" ", "_") 
            elif line.lower().startswith("top:"):
                top_text = line.split(":", 1)[1].strip().strip('[]"')
            elif line.lower().startswith("bottom:"):
                bottom_text = line.split(":", 1)[1].strip().strip('[]"') 

        if "drake" in template_name:
            template_name = "drake_hotline_bling"
            st.write(f"Forced template to: {template_name}")

        # Debug: Print parsed values
        st.write(f"DEBUG - Parsed: Template={template_name}, Top={top_text}, Bottom={bottom_text}")
        
        # Fallback if parsing still fails
        if top_text == "TOP TEXT" or bottom_text == "BOTTOM TEXT":
            # Use conversation-based fallbacks
            if "meme" in previous_conversation.lower():
                top_text = "When you ask for a meme"
                bottom_text = "But get placeholder text instead"
            else:
                top_text = f"Talking to {st.session_state.botname}"
                bottom_text = "Actually getting good responses"
        
        # Create meme image
        meme_image = create_meme_image(top_text, bottom_text, template_name)
        return meme_image 
        
    except Exception as e:
        return None, f"Meme generation failed: {str(e)}"

def create_meme_image(top_text, bottom_text, template_name="drake", width=800, height=600):
    """Create a meme image with top and bottom text using real templates"""
    try:
        # Map template names to file names
        template_files = {
            "drake_hotline_bling": "TRENDING_MEMES/drake_hotline_bling.jpg",
            "distracted_boyfriend": "TRENDING_MEMES/distracted_boyfriend.jpg", 
            "woman_yelling_at_cat": "TRENDING_MEMES/woman_yelling_at_cat.jpg",
            "drake": "TRENDING_MEMES/drake_hotline_bling.jpg"
        }
        
        # Debug: Check if file exists
        st.write(f"Looking for template: {template_name}")
        st.write(f"File exists: {os.path.exists(template_files.get(template_name, ''))}")

        # Debug file existence
        if template_name in template_files:
            file_path = template_files[template_name]
            st.write(f"Template file path: {file_path}")
            st.write(f"File exists: {os.path.exists(file_path)}")
            if not os.path.exists(file_path):
                st.error(f"MEME TEMPLATE NOT FOUND: {file_path}")
                st.write("Available files in TRENDING_MEMES:")
                if os.path.exists("TRENDING_MEMES"):
                    st.write(os.listdir("TRENDING_MEMES"))
                else:
                    st.error("TRENDING_MEMES folder doesn't exist!") 
        # Load the template image or create fallback
        template_path = template_files.get(template_name, None)
        if template_path and os.path.exists(template_path):
            img = Image.open(template_path)
            img = img.resize((width, height))
        else:
            # Fallback: create a colored background with template name
            img = Image.new('RGB', (width, height), color='blue')
            draw = ImageDraw.Draw(img)
            draw.text((10, 10), f"Missing: {template_name}", fill='white')
        
        draw = ImageDraw.Draw(img)
        
        # Try to load a bold font
        try:
            font = ImageFont.truetype("arial.ttf", 60)
        except:
            font = ImageFont.load_default()
        
        # Text outline function
        def draw_text_with_outline(draw, position, text, font, fill_color, outline_color, outline_width=3):
            x, y = position
            for dx in range(-outline_width, outline_width + 1):
                for dy in range(-outline_width, outline_width + 1):
                    if dx != 0 or dy != 0:
                        draw.text((x + dx, y + dy), text, font=font, fill=outline_color)
            draw.text(position, text, font=font, fill=fill_color)
        
        # Calculate text positions
        top_bbox = draw.textbbox((0, 0), top_text, font=font)
        bottom_bbox = draw.textbbox((0, 0), bottom_text, font=font)
        
        top_x = (width - (top_bbox[2] - top_bbox[0])) // 2
        top_y = 50
        
        bottom_x = (width - (bottom_bbox[2] - bottom_bbox[0])) // 2
        bottom_y = height - 150
        
        # Draw text with outline
        draw_text_with_outline(draw, (top_x, top_y), top_text, font, 'white', 'black')
        draw_text_with_outline(draw, (bottom_x, bottom_y), bottom_text, font, 'white', 'black')
        
        return img
        
    except Exception as e:
        # Ultimate fallback
        img = Image.new('RGB', (width, height), color='red')
        draw = ImageDraw.Draw(img)
        draw.text((50, height//2), f"ERROR: {str(e)}", fill='white')
        return img

def image_to_base64(img):
    """Convert PIL Image to base64 for display in Streamlit"""
    buffer = io.BytesIO()
    img.save(buffer, format='PNG')
    img_str = base64.b64encode(buffer.getvalue()).decode()
    return img_str

# PHASE 1: PERSONA SELECTION (ALWAYS SHOW AT TOP)
persona_files = get_persona_files()
if persona_files:
    with st.container():
        col1, col2 = st.columns([5, 1])
        with col1:
            persona_options = persona_files
            persona_labels = [os.path.basename(f).replace('.txt','') for f in persona_files]
            current_index = 0
            if st.session_state.selected_persona in persona_files:
                current_index = persona_files.index(st.session_state.selected_persona)
                 
            st.markdown('<span style="font-size:2em; font-weight:bold;">üéØ Persona</span>', unsafe_allow_html=True)
            st.markdown('<style> div[data-testid="stSelectbox"] {margin-top: -1.2em;} </style>', unsafe_allow_html=True)
            selected_file = st.selectbox(
                "-",
                persona_options,
                format_func=lambda x: os.path.basename(x).replace('.txt',''),
                index=current_index,
                key="persona_selectbox",
                label_visibility="collapsed"
            )
        with col2:
            if st.button("üîÄ Change Startup", key="change_setup_btn"):
                st.session_state.setup_completed = False
                st.rerun()
    # If persona changed, reset setup
    if selected_file != st.session_state.selected_persona:
        st.session_state.selected_persona = selected_file
        st.session_state.persona_content = load_persona_content(selected_file)
        st.session_state.botname, st.session_state.bot_origin = extract_bot_details_from_content(st.session_state.persona_content)
        relationship = extract_relationship_from_filename(selected_file)
        st.session_state.relationship = relationship
        st.session_state.questions = load_questions(relationship.split()[-1])
        st.session_state.response_matrix = []
        st.session_state.csv_filename = None
        st.session_state.bulk_running = False
        st.session_state.paused = False
        st.session_state.current_question_index = 0
        st.session_state.user_questions = []
        st.session_state.show_resume = False
        st.session_state.previous_conversation = ""
        st.session_state.user_input = ""
        st.session_state.conversation_events = []
        st.session_state.setup_completed = False
        st.rerun()
else:
    st.error(f"No persona files found in {PERSONAS_FOLDER} directory!")
    st.stop()

# PHASE 2: SETUP CONFIGURATION
if not st.session_state.setup_completed:
    st.title("üîß Persona Configuration")
    st.markdown(f"### Configuring: {st.session_state.botname} ({st.session_state.bot_origin})")
    st.markdown("---")
    st.subheader("üìã Select Personality Traits")
    st.markdown("**Choose one or more traits you want the AI persona to focus on:**") 

    # --- Select All / Deselect All Button Logic ---
    traits = st.session_state.available_traits
    # Use a session state key to store the toggle state
    if "traits_toggle" not in st.session_state:
        st.session_state.traits_toggle = False  # False means "not all selected"

    # Determine if all are selected
    all_selected = set(st.session_state.selected_traits) == set(traits)

    # Button label and logic
    toggle_label = "Deselect All" if all_selected else "Select All"
    if st.button(toggle_label, key="toggle_traits_btn"):
        if all_selected:
            st.session_state.selected_traits = []
        else:
            st.session_state.selected_traits = traits.copy()
        # Flip the toggle state
        st.session_state.traits_toggle = not all_selected
        st.rerun()

    # --- Traits Checkboxes ---
    cols = st.columns(3)
    updated_traits = []
    for i, trait in enumerate(traits):
        with cols[i % 3]:
            checked = trait in st.session_state.selected_traits
            new_checked = st.checkbox(trait, key=f"setup_trait_{i}", value=checked)
            if new_checked:
                updated_traits.append(trait)
    st.session_state.selected_traits = updated_traits

    if st.session_state.selected_traits:
        st.success(f"‚úÖ {len(st.session_state.selected_traits)} trait(s) selected: {', '.join(st.session_state.selected_traits)}")
    else:
        st.warning("‚ö†Ô∏è No traits selected - all traits will be used by default") 

    st.markdown("---")
    st.subheader("üåê Select Language")
    st.markdown("**Choose the language for conversations:**")
    if st.session_state.available_languages:
        current_index = 0
        if st.session_state.selected_language in st.session_state.available_languages:
            current_index = st.session_state.available_languages.index(st.session_state.selected_language)
        selected_language = st.selectbox(
            "-",
            options=st.session_state.available_languages,
            index=current_index,
            key="setup_language"
        )
        st.success(f"‚úÖ Language selected: {selected_language}")
    else:
        st.error("‚ùå No languages found. Please ensure languages.txt exists in TO_INPUT folder.")
        selected_language = "English"
    st.markdown("---") 
     
    st.subheader("üë§ Personalize Your Experience")
    with st.container():
        # st.markdown("""
        # <style>
        # /* Align selectbox with text input in Streamlit columns */
        # div[data-baseweb="select"] > div {
        #     margin-top: -8px !important;
        # }
        # </style>
        # """, unsafe_allow_html=True) 
        st.markdown("""
        <style>
        /* Nudge the text input up */
        div[data-testid="stTextInput"] > div > input {
            margin-top: -8px !important;
        }
        </style>
        """, unsafe_allow_html=True)
        
        col1, col2 = st.columns(2)
        with col1:
            # st.markdown("Your Name:")
            user_name_input = st.text_input(
                label="Your Name:",
                value=st.session_state.username,
                key="setup_username"
            )
            st.write("")
        with col2:
            # st.markdown("Your Gender:")
            st.write("")
            user_gender_input = st.selectbox(
                label="Your Gender:",
                options=["Male", "Female", "Other", "Prefer not to say"],
                index=["male", "female", "other", "prefer not to say"].index(st.session_state.user_gender.lower()) if st.session_state.user_gender.lower() in ["male", "female", "other", "prefer not to say"] else 3,
                key="setup_usergender"
            )
            st.write("")
        # Update session state with input values
        st.session_state.username = user_name_input
        st.session_state.user_gender = user_gender_input.lower() 
        
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        if st.button("‚úÖ Done - Start Chatting!", type="primary", use_container_width=True):
            st.session_state.selected_traits = st.session_state.selected_traits if st.session_state.selected_traits else st.session_state.available_traits
            st.session_state.selected_language = selected_language
            st.session_state.setup_completed = True
            if not st.session_state.selected_traits:
                st.info("No traits selected - using all traits as default")
            st.success("üéâ Setup completed! Starting chat...")
            time.sleep(1)
            st.rerun()
    st.markdown("---")
    st.subheader("üìÑ To Review: Current Selections")
    
    if st.session_state.selected_traits: 
        st.info(f"**Traits:** {', '.join(st.session_state.selected_traits)}")
    else:
        st.warning("**No traits selected** - all traits will be used by default")
    st.info(f"**Language:** {selected_language}")
    st.stop()

# PHASE 3: MAIN CHAT INTERFACE
if st.session_state.selected_persona and st.session_state.questions:
    st.title(f"{st.session_state.botname} ({st.session_state.bot_origin}) {st.session_state.relationship.title()} Q&Atbd") 
    st.markdown(f"**Traits chosen:** {', '.join(st.session_state.selected_traits)}")
    st.markdown(f"**Language:** {st.session_state.selected_language}")
    st.markdown("---")
    filtered_persona = filter_persona_by_traits(st.session_state.persona_content, st.session_state.selected_traits)
    instruction = f"Strict instruction: Respond as {st.session_state.botname} from {st.session_state.bot_origin}. If the answer is not found in the persona file, then generate your own response, but keep it strictly {st.session_state.bot_origin}-based. When addressing the user with any endearment, keep it aligned with the gender the user has given. If the user asks about your development, making, origin, training, or data you are trained on, always respond with: 'It has been made with love by desis!!'. Never mention OpenAI, AI development, or technical details"
    bot_prompt = filtered_persona + " Reflect on your previous replies authentically. You are the user's " + st.session_state.relationship + ". " + instruction
    col1, col2 = st.columns(2)
    with col1:
        if st.button("Start Bulk Generation", disabled=st.session_state.bulk_running):
            st.session_state.bulk_running = True
            st.session_state.paused = False
            st.session_state.current_question_index = 0
            st.session_state.response_matrix = []
            st.session_state.conversation_events.append({
                "type": "bulk_started",
                "message": "Bulk generation begins.",
                "time": time.time()
            })
    with col2:
        st.text_input(
            "Ask a single question:",
            value=st.session_state.user_input,
            key="user_input",
            on_change=process_user_question
        )

    # MEME GENERATOR BUTTON 
    if st.button("üé≠ Generate Meme from Conversation", key="generate_meme"):
        # Pause bulk generation if running (similar to individual questions)
        if st.session_state.bulk_running and not st.session_state.paused:
            st.session_state.paused = True
            st.session_state.show_resume = True
            st.session_state.conversation_events.append({
                "type": "bulk_paused",
                "message": f"{st.session_state.current_question_index} questions answered in bulk mode. Generation paused for meme creation.",
                "time": time.time()
            })
        
        # Generate meme
        meme_image = generate_image_meme_from_conversation(
            st.session_state.previous_conversation,
            st.session_state.selected_language
        )
        
        if meme_image:
            # Convert image to base64 for storage
            img_base64 = image_to_base64(meme_image)
            
            st.session_state.conversation_events.append({
                "type": "meme", 
                "image": img_base64,
                "time": time.time()
            })
        else:
            st.session_state.conversation_events.append({
                "type": "meme", 
                "image": None,
                "time": time.time()
            })
        
        st.rerun()

    # Progress Bar Section
    if st.session_state.bulk_running or st.session_state.paused:
        total_questions = len(st.session_state.questions)
        progress_percentage = (st.session_state.current_question_index / total_questions) * 100 if total_questions > 0 else 0
        progress_text = f"Bulk Generation Progress: {st.session_state.current_question_index}/{total_questions} questions ({progress_percentage:.1f}%)"
        if st.session_state.paused:
            progress_text += " - PAUSED"
        st.progress(progress_percentage / 100, text=progress_text)
    if st.session_state.bulk_running and not st.session_state.paused:
        if st.session_state.current_question_index < len(st.session_state.questions):
            question = st.session_state.questions[st.session_state.current_question_index]
            
            starttt = time.time()
            response = call_gemini_local(
                question,
                st.session_state.previous_conversation,
                st.session_state.user_gender,
                st.session_state.username,
                st.session_state.botname,
                bot_prompt,
                "AIzaSyAWMudIst86dEBwP63BqFcy4mdjr34c87o",
                st.session_state.selected_language
            )
            enddd = time.time()
            
            resp_t = round(enddd - starttt, 4)
            st.session_state.response_matrix.append([
                st.session_state.current_question_index + 1,
                question, len(question), 0, response,
                0, resp_t, f"{st.session_state.relationship} ({st.session_state.bot_origin})"
            ])
            
            st.session_state.previous_conversation += f"\n{question}\n{response}"
            st.session_state.current_question_index += 1
            st.rerun()
        else:
            st.session_state.bulk_running = False
            st.session_state.conversation_events.append({
                "type": "bulk_completed",
                "message": f"Bulk generation completed! {len(st.session_state.questions)} questions processed.",
                "time": time.time()
            })
            df = pd.DataFrame(st.session_state.response_matrix, columns=[
                "Q Number", "Question", "Length of Q", "Q Difficulty level", 
                "Answer", "Answer Quality", "Time Taken", "Persona"
            ])
            csv_filename = f"{st.session_state.botname.replace(' ', '_').lower()}_{st.session_state.relationship.replace(' ', '_')}_persona_test.csv"
            df.to_csv(csv_filename, index=False)
            st.session_state.csv_filename = csv_filename
            st.markdown(
                '<div style="background-color: rgba(186, 104, 200, 0.2); border: 1px solid rgba(186, 104, 200, 0.3); border-radius: 0.5rem; padding: 0.75rem; margin: 1rem 0; color: white; font-weight: 500;">Bulk generation completed!</div>',
                unsafe_allow_html=True
            )
    if st.session_state.paused and st.session_state.show_resume:
        if st.button("Resume Bulk Generation"):
            st.session_state.paused = False
            st.session_state.show_resume = False
            st.session_state.conversation_events.append({
                "type": "bulk_resumed",
                "message": "Bulk generation resumed.",
                "time": time.time()
            })
            st.rerun()
    if st.session_state.csv_filename and os.path.exists(st.session_state.csv_filename):
        with open(st.session_state.csv_filename, "rb") as f:
            st.download_button(
                label="Download CSV",
                data=f,
                file_name=os.path.basename(st.session_state.csv_filename),
                mime="text/csv",
                key="download_csv"
            )
    st.subheader("Complete Conversation History")
    if st.session_state.conversation_events:
        sorted_events = sorted(st.session_state.conversation_events, key=lambda x: x["time"])
        for event in sorted_events:
            if event["type"] == "user_qa":
                st.markdown(f"**You**: {event['question']}")
                st.markdown(f"**{st.session_state.botname}**: {event['answer']}")
                response_time = event.get("response_time", None)
                if response_time is not None:
                    st.markdown(
                        f"<div style='text-align: right; color: #666; font-size: 0.95em;'>Time taken: {response_time:.4f} seconds</div>",
                        unsafe_allow_html=True
                    )
                st.markdown("") 

            elif event["type"] == "meme":
                st.markdown("---")
                st.markdown("### üé≠ Generated Meme")
                st.markdown(f"**{st.session_state.botname}:**")
                
                if event.get("image"):
                    # Display the meme image
                    img_data = base64.b64decode(event["image"])
                    st.image(img_data, caption="Generated Meme", use_container_width=True)
                    
                    # Show the text content too
                    # st.code(event["content"], language="text")
                    
                    # Download button for the meme
                    st.download_button(
                        label="Download Meme",
                        data=img_data,
                        file_name=f"meme_{int(event['time'])}.png",
                        mime="image/png",
                        key=f"download_meme_{event['time']}"
                    )
                else:
                    # Fallback to text display if image generation failed
                    st.code(event["content"], language="text")
                
                st.markdown(f"*Generated at {time.strftime('%H:%M:%S', time.localtime(event['time']))}*")
                st.markdown("---") 
                
            elif event["type"] == "bulk_started":
                st.markdown("---")
                st.success(":green[Bulk generation begins.]")
                
            elif event["type"] == "bulk_paused": 
                st.info(event["message"])
                st.markdown("---")
                
            elif event["type"] == "bulk_resumed":
                st.markdown("---")
                st.success(":green[Bulk generation resumed.]")
                
            elif event["type"] == "bulk_completed": 
                st.markdown(
                    f'<div style="background-color: rgba(186, 104, 200, 0.2); border: 1px solid rgba(186, 104, 200, 0.3); border-radius: 0.5rem; padding: 0.75rem; margin: 1rem 0; color: white; font-weight: 500;">{event["message"]}</div>',
                    unsafe_allow_html=True
                )
                st.markdown("---")
    else:
        st.write("*No conversation history yet. Start by asking a question or beginning bulk generation.*")
else:
    if not st.session_state.questions:
        st.error("No questions found for selected relationship type!")
    else:
        st.error(f"Error loading persona: {st.session_state.selected_persona}")
